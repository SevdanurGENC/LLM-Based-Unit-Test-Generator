# ğŸ§ª **LLM-Based Unit Test Generator**

*Automated unit test generation and evaluation using generative AI (GPT-4)*

## ğŸ“– Overview

This repository provides an implementation of an **LLM-based software unit test generation tool**, developed as part of the study *â€œSoftware Unit Test Automation with LLM-Based Generative AI: Evaluating Test Quality through Code Coverage and Edge-Case Analysisâ€*.
It automates the generation of unit tests for Python functions using **GPT-4**, executes the generated tests, analyzes their correctness and edge-case coverage, and reports code coverage using `pytest` and `coverage.py`.

The tool offers a repeatable, efficient pipeline for test automation and is designed for integration into modern software development workflows.

---

## ğŸš€ Features

âœ… Generates unit test files in **pytest** format using GPT-4.

âœ… Covers normal and **edge-case scenarios** (e.g., division by zero, invalid inputs).

âœ… Executes tests automatically and validates correctness.

âœ… Measures and reports **code coverage**.

âœ… Produces HTML and tabular reports.

âœ… Fully automated and reproducible process.

---

## ğŸ–¼ï¸ Demo Workflow

1ï¸âƒ£ Provide Python source code with functions to be tested.

2ï¸âƒ£ The tool parses the code and extracts target functions.

3ï¸âƒ£ Sends each function to GPT-4 with a structured prompt.

4ï¸âƒ£ Saves the generated tests in `tests/` directory.

5ï¸âƒ£ Runs the tests using `pytest` and analyzes results.

6ï¸âƒ£ Generates and saves code coverage reports in HTML format.

---

## ğŸ“‚ Directory Structure

```
genai_test_generator/
â”œâ”€â”€ math_utils.py            # Python source code with sample functions
â”œâ”€â”€ tests/                    # Directory for generated test files
â”‚   â”œâ”€â”€ test_add_llm.py
â”‚   â”œâ”€â”€ test_divide_llm.py
â”‚   â””â”€â”€ test_factorial_llm.py
â”œâ”€â”€ reports/                  # HTML code coverage reports
â”œâ”€â”€ main.py                   # Main script to run the pipeline
â”œâ”€â”€ requirements.txt          # Required Python packages
```

---

## âš™ï¸ Installation

Clone the repository and install dependencies:

```bash
git clone https://github.com/yourusername/llm-unit-test-generator.git
cd llm-unit-test-generator
pip install -r requirements.txt
```

---

## ğŸ§‘â€ğŸ’» Usage

Run the main script:

```bash
python main.py
```

By default, it will:

* Read `math_utils.py`
* Generate tests for each function
* Run tests
* Measure coverage
* Save results

You can customize the source file or output directories by modifying `main.py`.

---

## ğŸ“Š Output

âœ… Test result summary (pass/fail).

âœ… Code coverage percentage.

âœ… Edge-case inclusion indicators.

âœ… HTML coverage report in `/reports`.

---

## ğŸ“š Future Work

We plan to extend this tool for **more complex and large-scale codebases**, evaluating the performance of LLM-based test generation on real-world software modules and multi-function dependencies. The current pipeline will serve as a foundation for scaling up and experimenting with advanced generative AI models.

---

## ğŸ¤ Contribution

Feel free to fork, contribute, or open issues for improvements.
If you use this tool in academic work, please cite the related paper.

---

## ğŸ“„ License

MIT License â€” see `LICENSE` file.

---

ğŸ“¬ **Contact:**
For questions or feedback:

[sevdanur.genc@balikesir.edu.tr](mailto:sevdanur.genc@balikesir.edu.tr)

[furkan.ceylan@balikesir.edu.tr](mailto:furkan.ceylan@balikesir.edu.tr)

